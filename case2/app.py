from fastapi import FastAPI, Body, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import chromadb
from sentence_transformers import SentenceTransformer
from chatbot_logic import ChatbotLogic
from embeddings import EmbeddingUtil
from llm_handler import MistralHandler

app = FastAPI()
chatbot = ChatbotLogic()
embedding_util = EmbeddingUtil()
llm = MistralHandler()

# ChromaDB ayarlarÄ±
chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection("sss_collection")

# CORS ayarlarÄ±
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ðŸ”¹ Pydantic modeli (Swagger UI'da body alanÄ±nÄ± gÃ¶sterir!)
class ChatRequest(BaseModel):
    session_id: str
    message: str

class ChatResponse(BaseModel):
    reply: str
    source: str = "database"  # "database" veya "llm"

@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    try:
        session = chatbot.get_session(req.session_id)
        # ilk Ã¶nce chatbot akÄ±ÅŸÄ±nÄ± dene
        try:
            chatbot_response = chatbot.process_message(req.session_id, req.message)
            if chatbot_response != "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu. BaÅŸvuruyu yeniden baÅŸlatabilir miyiz?":
                return ChatResponse(reply=chatbot_response, source="database")
        except Exception as flow_error:
            print(f"Flow error: {str(flow_error)}")
        # chatbot akÄ±ÅŸÄ± baÅŸarÄ±sÄ±z olduysa, SSS'lerde ara
        query_embedding = embedding_util.get_embedding(req.message)
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=1
        )
        # SSS'lerde yanÄ±t bulunursa, onu dÃ¶ndÃ¼r
        if results["documents"] and results["documents"][0]:
            sss_response = results["documents"][0][0]
            return ChatResponse(reply=sss_response, source="database")
        
        context = chatbot.get_session_context(req.session_id)
        prompt = llm.format_prompt(req.message, context)
        llm_response = llm.generate_response(prompt)
        
        return ChatResponse(reply=llm_response, source="llm")
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
